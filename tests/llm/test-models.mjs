/**
 * æµ‹è¯•ä¸åŒ GLM æ¨¡åž‹ç‰ˆæœ¬
 * ç”¨æ³•: node scripts/test-models.mjs
 */

const GLM_API_KEY = '6b35d40fa78f134ba53d669abf0d26f5.udk8D9gGzss6l9o5'
const GLM_API_URL = 'https://open.bigmodel.cn/api/paas/v4/chat/completions'

// è¦æµ‹è¯•çš„æ¨¡åž‹åˆ—è¡¨
const MODELS_TO_TEST = [
  'glm-4',
  'glm-4-plus',
  'glm-4-air',
  'glm-4-airx',
  'glm-4-flash',
  'glm-4v',
  'glm-4v-plus',
  'glm-4-alltools',
]

async function testModel(modelName) {
  console.log(`\nðŸ§ª æµ‹è¯•æ¨¡åž‹: ${modelName}`)
  console.log('-'.repeat(50))
  
  const startTime = Date.now()
  
  try {
    const requestBody = {
      model: modelName,
      messages: [
        { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªè‹±è¯­å¯¹è¯åŠ©æ‰‹ã€‚è¯·ç”¨è‹±æ–‡å›žå¤ã€‚' },
        { role: 'user', content: 'Hello! How are you?' }
      ],
      temperature: 0.7,
      max_tokens: 100,
      top_p: 0.95,
    }
    
    const response = await fetch(GLM_API_URL, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${GLM_API_KEY}`,
      },
      body: JSON.stringify(requestBody),
    })

    const duration = Date.now() - startTime

    if (!response.ok) {
      const errorText = await response.text().catch(() => '')
      console.log(`   âŒ API é”™è¯¯: ${response.status}`)
      console.log(`   é”™è¯¯è¯¦æƒ…: ${errorText.substring(0, 200)}`)
      return { model: modelName, success: false, error: `HTTP ${response.status}`, duration }
    }

    const data = await response.json()
    const content = data.choices?.[0]?.message?.content || ''
    const usage = data.usage
    
    console.log(`   çŠ¶æ€: ${content ? 'âœ… æˆåŠŸ' : 'âŒ ç©ºå“åº”'}`)
    console.log(`   è€—æ—¶: ${duration}ms`)
    console.log(`   Token: ${usage?.total_tokens || 'N/A'} (æç¤º: ${usage?.prompt_tokens || 'N/A'}, ç”Ÿæˆ: ${usage?.completion_tokens || 'N/A'})`)
    console.log(`   å“åº”: ${content.substring(0, 100)}${content.length > 100 ? '...' : ''}`)
    
    return { 
      model: modelName, 
      success: !!content, 
      content: content.substring(0, 200),
      duration,
      usage
    }
  } catch (error) {
    console.log(`   âŒ å¼‚å¸¸: ${error.message}`)
    return { model: modelName, success: false, error: error.message, duration: Date.now() - startTime }
  }
}

async function runTests() {
  console.log('========================================')
  console.log('ðŸ¤– GLM æ¨¡åž‹ç‰ˆæœ¬æµ‹è¯•')
  console.log('========================================')
  
  const results = []
  
  for (const model of MODELS_TO_TEST) {
    const result = await testModel(model)
    results.push(result)
    
    // ç­‰å¾… 1 ç§’é¿å…é¢‘çŽ‡é™åˆ¶
    await new Promise(resolve => setTimeout(resolve, 1000))
  }
  
  // æ€»ç»“
  console.log('\n========================================')
  console.log('ðŸ“Š æµ‹è¯•ç»“æžœæ€»ç»“')
  console.log('========================================')
  
  const successful = results.filter(r => r.success)
  const failed = results.filter(r => !r.success)
  
  console.log(`\nâœ… å¯ç”¨æ¨¡åž‹ (${successful.length}/${results.length}):`)
  successful.forEach(r => {
    console.log(`   - ${r.model} (${r.duration}ms, ${r.usage?.total_tokens || 'N/A'} tokens)`)
  })
  
  if (failed.length > 0) {
    console.log(`\nâŒ ä¸å¯ç”¨æ¨¡åž‹ (${failed.length}/${results.length}):`)
    failed.forEach(r => {
      console.log(`   - ${r.model}: ${r.error || 'ç©ºå“åº”'}`)
    })
  }
  
  // æŽ¨èæ¨¡åž‹
  console.log('\nðŸ’¡ æŽ¨èæ¨¡åž‹:')
  if (successful.length > 0) {
    // æŒ‰ token ä½¿ç”¨æŽ’åºï¼ŒæŽ¨èæœ€ç»æµŽçš„
    const sortedByTokens = [...successful].sort((a, b) => 
      (a.usage?.total_tokens || Infinity) - (b.usage?.total_tokens || Infinity)
    )
    console.log(`   æœ€ç»æµŽ: ${sortedByTokens[0]?.model}`)
    
    // æŒ‰é€Ÿåº¦æŽ’åº
    const sortedBySpeed = [...successful].sort((a, b) => a.duration - b.duration)
    console.log(`   æœ€å¿«: ${sortedBySpeed[0]?.model}`)
  }
  
  console.log('\n========================================')
}

runTests().catch(console.error)
