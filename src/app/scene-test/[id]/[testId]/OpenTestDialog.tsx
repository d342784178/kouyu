'use client'

import { useState, useEffect, useRef, useCallback } from 'react'
import { AnimatePresence } from 'framer-motion'
import TestAnalysis from './TestAnalysis'
import LoadingSpinner from './components/LoadingSpinner'
import {
  AnalyzingView,
  RoleSelectionView,
  InitializingView,
  ActiveChatView,
  CompletedView,
  Message,
  TestStatus,
  QuestionAnalysis,
  DifficultyLevel,
  OpenTestDialogProps
} from '../../components/open-test'

export default function OpenTestDialog({
  sceneId,
  testId,
  testContent,
  currentIndex,
  totalTests,
  onComplete,
  autoStart = false,
  onStatusChange
}: OpenTestDialogProps) {
  // çŠ¶æ€ç®¡ç†
  const [status, setStatus] = useState<TestStatus>('idle')
  
  // çŠ¶æ€å˜åŒ–æ—¶é€šçŸ¥çˆ¶ç»„ä»¶
  useEffect(() => {
    onStatusChange?.(status)
  }, [status, onStatusChange])
  const [messages, setMessages] = useState<Message[]>([])
  const [currentRound, setCurrentRound] = useState(0)
  const [maxRounds] = useState(5)
  const [isRecording, setIsRecording] = useState(false)
  const [playingMessageIndex, setPlayingMessageIndex] = useState<number | null>(null)
  const [recognition, setRecognition] = useState<any>(null)
  const [audioElement, setAudioElement] = useState<HTMLAudioElement | null>(null)
  const [loading, setLoading] = useState(false)
  const [isGeneratingResponse, setIsGeneratingResponse] = useState(false)
  const [error, setError] = useState<string>('')
  const [showAnalysis, setShowAnalysis] = useState(false)
  const [isRoundLimitReached, setIsRoundLimitReached] = useState(false)

  // æ–°å¢çŠ¶æ€
  const [questionAnalysis, setQuestionAnalysis] = useState<QuestionAnalysis | null>(null)
  const [selectedRole, setSelectedRole] = useState<string>('')
  const [difficultyLevel, setDifficultyLevel] = useState<DifficultyLevel>('medium')
  const [voiceEnabled, setVoiceEnabled] = useState<boolean>(true)

  // ä½¿ç”¨ ref æ¥å­˜å‚¨æœ€æ–°çš„çŠ¶æ€ï¼Œé¿å…é—­åŒ…é—®é¢˜
  const messagesRef = useRef<Message[]>([])
  const currentRoundRef = useRef<number>(0)
  const hasRecognitionResultRef = useRef<boolean>(false)
  const recordingTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const finalTranscriptRef = useRef<string>('')
  const interimTranscriptRef = useRef<string>('')

  // åŒæ­¥çŠ¶æ€åˆ° ref
  useEffect(() => {
    messagesRef.current = messages
  }, [messages])

  useEffect(() => {
    currentRoundRef.current = currentRound
  }, [currentRound])

  const messagesEndRef = useRef<HTMLDivElement>(null)

  // åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
  useEffect(() => {
    if ('webkitSpeechRecognition' in window) {
      const SpeechRecognition = (window as any).webkitSpeechRecognition
      const recognition = new SpeechRecognition()
      recognition.continuous = true
      recognition.interimResults = true
      recognition.lang = 'en-US'

      // æ¸…é™¤åœé¡¿æ£€æµ‹å®šæ—¶å™¨
      const clearSilenceTimeout = () => {
        if (silenceTimeoutRef.current) {
          clearTimeout(silenceTimeoutRef.current)
          silenceTimeoutRef.current = null
        }
      }

      // è®¾ç½®åœé¡¿æ£€æµ‹å®šæ—¶å™¨ - 800æ¯«ç§’æ— è¯­éŸ³è¾“å…¥åˆ™è‡ªåŠ¨åœæ­¢
      const setSilenceTimeout = () => {
        clearSilenceTimeout()
        silenceTimeoutRef.current = setTimeout(() => {
          console.log('æ£€æµ‹åˆ°800æ¯«ç§’åœé¡¿ï¼Œè‡ªåŠ¨åœæ­¢å½•éŸ³')
          // å¦‚æœæœ‰è¯†åˆ«ç»“æœï¼Œä½¿ç”¨æœ€ç»ˆç»“æœ
          const transcript = finalTranscriptRef.current || interimTranscriptRef.current
          if (transcript.trim()) {
            hasRecognitionResultRef.current = true
            handleVoiceInput(transcript.trim())
          }
          // åœæ­¢å½•éŸ³
          try {
            recognition.stop()
          } catch (err) {
            console.error('è‡ªåŠ¨åœæ­¢å½•éŸ³å¤±è´¥:', err)
          }
        }, 800) // 800æ¯«ç§’åœé¡¿æ£€æµ‹
      }

      recognition.onresult = (event: any) => {
        let finalTranscript = ''
        let interimTranscript = ''

        // éå†æ‰€æœ‰ç»“æœ
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript
          if (event.results[i].isFinal) {
            finalTranscript += transcript
          } else {
            interimTranscript += transcript
          }
        }

        // æ›´æ–°å­˜å‚¨çš„è½¬å½•æ–‡æœ¬
        if (finalTranscript) {
          finalTranscriptRef.current += finalTranscript
        }
        if (interimTranscript) {
          interimTranscriptRef.current = interimTranscript
        }

        // æ¯æ¬¡æ£€æµ‹åˆ°è¯­éŸ³è¾“å…¥æ—¶é‡ç½®åœé¡¿å®šæ—¶å™¨
        setSilenceTimeout()

        // å¦‚æœæœ‰æœ€ç»ˆç»“æœï¼Œæ ‡è®°ä¸ºæœ‰è¯†åˆ«ç»“æœ
        if (finalTranscript) {
          hasRecognitionResultRef.current = true
        }
      }

      recognition.onerror = (event: any) => {
        console.error('è¯­éŸ³è¯†åˆ«é”™è¯¯:', event.error)
        clearSilenceTimeout()
        
        let errorMessage = 'è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•'
        
        // æ ¹æ®é”™è¯¯ç±»å‹æä¾›æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
        switch (event.error) {
          case 'audio-capture':
            errorMessage = 'æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£æƒé™å’Œè®¾å¤‡'
            break
          case 'not-allowed':
            errorMessage = 'éº¦å…‹é£æƒé™è¢«æ‹’ç»ï¼Œè¯·åœ¨æµè§ˆå™¨è®¾ç½®ä¸­å…è®¸è®¿é—®éº¦å…‹é£'
            break
          case 'network':
            errorMessage = 'ç½‘ç»œé”™è¯¯ï¼Œè¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨'
            break
          case 'aborted':
            // ç”¨æˆ·ä¸»åŠ¨å–æ¶ˆï¼Œä¸æ˜¾ç¤ºé”™è¯¯
            errorMessage = ''
            break
          case 'no-speech':
            errorMessage = 'æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•'
            break
          default:
            errorMessage = `è¯­éŸ³è¯†åˆ«å¤±è´¥: ${event.error}`
        }
        
        if (errorMessage) {
          setError(errorMessage)
        }
        hasRecognitionResultRef.current = false
        setIsRecording(false)
      }

      recognition.onend = () => {
        clearSilenceTimeout()
        
        // æ£€æŸ¥æ˜¯å¦æœ‰è¯†åˆ«ç»“æœ
        if (!hasRecognitionResultRef.current) {
          // æ²¡æœ‰è¯†åˆ«åˆ°è¯­éŸ³ï¼Œæ˜¾ç¤ºæç¤º
          setError('æœªæ£€æµ‹åˆ°è¯­éŸ³ï¼Œè¯·é‡è¯•')
        }
        
        // é‡ç½®çŠ¶æ€
        hasRecognitionResultRef.current = false
        finalTranscriptRef.current = ''
        interimTranscriptRef.current = ''
        setIsRecording(false)
        
        // æ¸…é™¤è¶…æ—¶å®šæ—¶å™¨
        if (recordingTimeoutRef.current) {
          clearTimeout(recordingTimeoutRef.current)
          recordingTimeoutRef.current = null
        }
      }

      setRecognition(recognition)
    }
  }, [])

  // æ»šåŠ¨åˆ°æœ€æ–°æ¶ˆæ¯
  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' })
  }, [messages])

  // è‡ªåŠ¨å¼€å§‹åˆ†æï¼ˆå¦‚æœè®¾ç½®äº†autoStartï¼‰
  useEffect(() => {
    if (autoStart && testContent && status === 'idle') {
      console.log('[OpenTestDialog] è‡ªåŠ¨å¼€å§‹åˆ†æé¢˜ç›®')
      // ç›´æ¥ä½¿ç”¨ testContent æ„å»º questionAnalysisï¼Œä¸éœ€è¦è°ƒç”¨ API
      buildQuestionAnalysisFromContent()
    }
  }, [autoStart, testContent])

  // ä» testContent æ„å»ºé¢˜ç›®åˆ†æç»“æœ
  const buildQuestionAnalysisFromContent = () => {
    setStatus('analyzing')
    
    // ä» testContent ä¸­æå–è§’è‰²ä¿¡æ¯
    const userRoles = testContent.roles.map((role, index) => ({
      id: `role_${index}`,
      name: role.name,
      description: role.description,
      emoji: 'ğŸ™‹'
    }))

    // æ„å»ºé¢˜ç›®åˆ†æç»“æœ
    const analysisResult: QuestionAnalysis = {
      sceneType: testContent.topic,
      sceneDescription: testContent.scenario_context,
      aiRole: {
        id: 'ai',
        name: testContent.roles[0]?.name || 'AIåŠ©æ‰‹',
        description: testContent.roles[0]?.description || 'å¯¹è¯åŠ©æ‰‹',
        emoji: 'ğŸ¤–'
      },
      userRoles: userRoles,
      dialogueGoal: testContent.description,
      suggestedTopics: ['æ—¥å¸¸è¯é¢˜', 'åœºæ™¯å¯¹è¯', 'è§’è‰²æ‰®æ¼”']
    }

    console.log('ä» testContent æ„å»ºçš„é¢˜ç›®åˆ†æç»“æœ:', analysisResult)
    setQuestionAnalysis(analysisResult)
    setStatus('role-selection')
  }

  // ç¡®è®¤è§’è‰²å’Œéš¾åº¦ç­‰çº§ï¼Œå¼€å§‹å¯¹è¯
  const confirmRoleAndDifficulty = async () => {
    console.log('confirmRoleAndDifficulty è¢«è°ƒç”¨', { selectedRole, hasRole: !!selectedRole })
    if (!selectedRole) {
      console.log('æœªé€‰æ‹©è§’è‰²ï¼Œè¿”å›')
      setError('è¯·é€‰æ‹©ä¸€ä¸ªè§’è‰²')
      return
    }

    setStatus('initializing')
    setError('')

    try {
      console.log('ç¡®è®¤è§’è‰²å’Œéš¾åº¦ç­‰çº§:', {
        selectedRole,
        difficultyLevel,
        questionAnalysis
      })

      // æ ¹æ®é€‰ä¸­çš„è§’è‰²IDæ‰¾åˆ°å¯¹åº”çš„è§’è‰²åç§°
      const selectedUserRole = questionAnalysis?.userRoles?.find(role => role.id === selectedRole)
      const userRoleName = selectedUserRole?.name || selectedRole

      // æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„è§’è‰²ï¼ŒåŠ¨æ€ç¡®å®šAIè§’è‰²ï¼ˆAIæ‰®æ¼”å¦ä¸€ä¸ªè§’è‰²ï¼‰
      const selectedRoleIndex = parseInt(selectedRole.replace('role_', ''))
      const aiRoleIndex = selectedRoleIndex === 0 ? 1 : 0
      const aiRoleName = testContent.roles[aiRoleIndex]?.name || 'AIåŠ©æ‰‹'

      console.log('è§’è‰²åˆ†é…:', { userRole: userRoleName, aiRole: aiRoleName })

      const initRequest = {
        sceneId,
        testId,
        scene: questionAnalysis?.sceneType,
        aiRole: aiRoleName,
        userRole: userRoleName,
        dialogueGoal: questionAnalysis?.dialogueGoal,
        difficultyLevel,
        suggestedOpening: testContent.suggested_opening
      }

      console.log('åˆå§‹åŒ–å¯¹è¯è¯·æ±‚:', JSON.stringify(initRequest, null, 2))

      const response = await fetch('/api/open-test/initiate', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(initRequest),
      })

      const data = await response.json()
      console.log('åˆå§‹åŒ–å¯¹è¯å“åº”:', JSON.stringify(data, null, 2))

      if (!response.ok) {
        throw new Error(data.error || 'åˆå§‹åŒ–å¯¹è¯å¤±è´¥')
      }

      const initialMessage: Message = {
        role: 'assistant',
        content: data.message || testContent.suggested_opening,
        audioUrl: data.audioUrl,
        timestamp: Date.now(),
      }

      console.log('ç”Ÿæˆçš„åˆå§‹æ¶ˆæ¯:', initialMessage)

      setMessages([initialMessage])
      setCurrentRound(1)
      setStatus('active')

      if (initialMessage.audioUrl) {
        console.log('è‡ªåŠ¨æ’­æ”¾åˆå§‹è¯­éŸ³:', initialMessage.audioUrl)
        setTimeout(() => {
          playAudio(initialMessage.audioUrl!, 0)
        }, 500)
      }
    } catch (err) {
      console.error('åˆå§‹åŒ–å¯¹è¯å¤±è´¥:', err)
      // ä½¿ç”¨ suggested_opening ä½œä¸ºå›é€€
      const initialMessage: Message = {
        role: 'assistant',
        content: testContent.suggested_opening,
        timestamp: Date.now(),
      }
      setMessages([initialMessage])
      setCurrentRound(1)
      setStatus('active')
    }
  }

  // ç”ŸæˆåŠ©æ‰‹æ¶ˆæ¯ï¼ˆç»§ç»­å¯¹è¯ï¼‰
  const generateAssistantMessage = async (
    prompt: string,
    conversationHistory: Message[],
    round: number
  ): Promise<{ message: Message; isEnd: boolean; isComplete?: boolean }> => {
    setLoading(true)

    try {
      console.log('=== å¼€å§‹ç”ŸæˆåŠ©æ‰‹æ¶ˆæ¯ï¼ˆç»§ç»­å¯¹è¯ï¼‰===')
      console.log('ç”¨æˆ·è¾“å…¥:', prompt)
      console.log('å®Œæ•´å¯¹è¯å†å²:', conversationHistory)
      console.log('å½“å‰è½®æ•°:', round)

      const updatedHistory = conversationHistory.map(msg => ({
        role: msg.role,
        content: msg.content
      }))

      // æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„è§’è‰²ï¼ŒåŠ¨æ€ç¡®å®šAIè§’è‰²ï¼ˆAIæ‰®æ¼”å¦ä¸€ä¸ªè§’è‰²ï¼‰
      const selectedRoleIndex = parseInt(selectedRole.replace('role_', ''))
      const aiRoleIndex = selectedRoleIndex === 0 ? 1 : 0
      const aiRoleName = testContent.roles[aiRoleIndex]?.name || 'AIåŠ©æ‰‹'

      const apiRequest = {
        sceneId,
        testId,
        conversation: updatedHistory,
        round,
        maxRounds,
        scene: questionAnalysis?.sceneType || testContent.topic,
        aiRole: aiRoleName,
        userRole: selectedRole || 'ç”¨æˆ·',
        dialogueGoal: questionAnalysis?.dialogueGoal || testContent.description,
        difficultyLevel
      }

      console.log('API è¯·æ±‚å‚æ•°:', JSON.stringify(apiRequest, null, 2))

      const response = await fetch('/api/open-test/continue', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(apiRequest),
      })

      const data = await response.json()
      console.log('API å“åº”çŠ¶æ€:', response.status)
      console.log('API å“åº”æ•°æ®:', JSON.stringify(data, null, 2))

      if (!response.ok) {
        console.error('å¤§æ¨¡å‹ API è°ƒç”¨å¤±è´¥:', data)
        throw new Error(data.error || 'å¤§æ¨¡å‹ API è°ƒç”¨å¤±è´¥')
      }

      const assistantMessage = {
        role: 'assistant' as const,
        content: data.message,
        audioUrl: data.audioUrl,
        timestamp: Date.now(),
      }

      console.log('ç”Ÿæˆçš„åŠ©æ‰‹æ¶ˆæ¯:', assistantMessage)
      console.log('æ˜¯å¦ç»“æŸå¯¹è¯:', data.isEnd || false)
      console.log('å¯¹è¯æ˜¯å¦å·²å®Œæˆ:', data.isComplete || false)

      return {
        message: assistantMessage,
        isEnd: data.isEnd || false,
        isComplete: data.isComplete || false
      }
    } catch (err) {
      console.error('ç”ŸæˆåŠ©æ‰‹æ¶ˆæ¯å¤±è´¥:', err)
      throw err
    } finally {
      setLoading(false)
      console.log('=== ç”ŸæˆåŠ©æ‰‹æ¶ˆæ¯å®Œæˆ ===')
    }
  }

  // å¤„ç†è¯­éŸ³è¾“å…¥
  const handleVoiceInput = async (transcript: string) => {
    if (!transcript.trim()) return

    const currentMessages = messagesRef.current
    const currentRoundValue = currentRoundRef.current

    console.log('=== å¼€å§‹å¤„ç†è¯­éŸ³è¾“å…¥ ===')
    console.log('å½“å‰æ¶ˆæ¯çŠ¶æ€:', currentMessages)
    console.log('ç”¨æˆ·è¾“å…¥:', transcript)
    console.log('å½“å‰è½®æ•°:', currentRoundValue)

    const userMessage: Message = {
      role: 'user',
      content: transcript,
      timestamp: Date.now(),
    }

    const updatedMessages = [...currentMessages, userMessage]
    setMessages(updatedMessages)
    console.log('æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åçš„å¯¹è¯å†å²:', updatedMessages)

    setIsGeneratingResponse(true)

    try {
      console.log('è°ƒç”¨generateAssistantMessageï¼Œä½¿ç”¨å®Œæ•´å¯¹è¯å†å²')
      const nextRound = currentRoundValue + 1
      console.log('ä¼ é€’çš„è½®æ•°:', nextRound)
      const { message: assistantMessage, isEnd, isComplete } = await generateAssistantMessage(
        transcript,
        updatedMessages,
        nextRound
      )
      console.log('æ”¶åˆ°åŠ©æ‰‹æ¶ˆæ¯:', assistantMessage)
      console.log('å¯¹è¯å®ŒæˆçŠ¶æ€:', isComplete)

      const completeHistory = [...updatedMessages, assistantMessage]
      console.log('æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åçš„å®Œæ•´å¯¹è¯å†å²:', completeHistory)

      setMessages(completeHistory)
      console.log('æ›´æ–°è½®æ•°:', nextRound)
      setCurrentRound(nextRound)

      if (assistantMessage.audioUrl) {
        console.log('è‡ªåŠ¨æ’­æ”¾AIè¯­éŸ³:', assistantMessage.audioUrl)
        setTimeout(() => {
          playAudio(assistantMessage.audioUrl!, completeHistory.length - 1)
        }, 500)
      }

      // æ£€æŸ¥å¯¹è¯æ˜¯å¦ç»“æŸï¼ˆç”±å¤§æ¨¡å‹åˆ¤æ–­å®Œæˆæˆ–è¾¾åˆ°æœ€å¤§è½®æ•°ï¼‰
      if (isEnd || nextRound >= maxRounds) {
        // æ— è®ºæ˜¯AIåˆ¤æ–­å®Œæˆè¿˜æ˜¯è¾¾åˆ°æœ€å¤§è½®æ•°ï¼Œéƒ½æ˜¾ç¤ºæäº¤è¯„æµ‹æŒ‰é’®
        if (isEnd) {
          console.log('å¯¹è¯ç›®æ ‡å·²å®Œæˆï¼Œç­‰å¾…ç”¨æˆ·æäº¤è¯„æµ‹')
        } else {
          console.log('è¾¾åˆ°æœ€å¤§è½®æ•°ï¼Œç­‰å¾…ç”¨æˆ·æäº¤è¯„æµ‹')
        }
        setIsRoundLimitReached(true)
      }
    } catch (err) {
      console.error('å¤„ç†è¯­éŸ³è¾“å…¥å¤±è´¥:', err)
      const errorMessage = err instanceof Error ? err.message : 'å¤„ç†è¾“å…¥å¤±è´¥ï¼Œè¯·é‡è¯•'
      alert(`GLM APIè°ƒç”¨å¤±è´¥: ${errorMessage}`)
      setError(errorMessage)
    } finally {
      setIsGeneratingResponse(false)
      console.log('=== è¯­éŸ³è¾“å…¥å¤„ç†å®Œæˆ ===')
    }
  }

  // å¼€å§‹å½•éŸ³
  const startRecording = useCallback(async () => {
    if (recognition && !isRecording && !loading) {
      try {
        // é‡ç½®è¯†åˆ«ç»“æœæ ‡å¿—å’Œè½¬å½•æ–‡æœ¬
        hasRecognitionResultRef.current = false
        finalTranscriptRef.current = ''
        interimTranscriptRef.current = ''
        setIsRecording(true)
        setError('')
        
        // å…ˆè¯·æ±‚éº¦å…‹é£æƒé™
        await navigator.mediaDevices.getUserMedia({ audio: true })
        
        recognition.start()

        // 30ç§’æœ€å¤§å½•éŸ³æ—¶é•¿é™åˆ¶
        recordingTimeoutRef.current = setTimeout(() => {
          console.log('è¾¾åˆ°æœ€å¤§å½•éŸ³æ—¶é•¿30ç§’ï¼Œè‡ªåŠ¨åœæ­¢')
          if (recognition) {
            try {
              // ä½¿ç”¨å½“å‰çš„è½¬å½•æ–‡æœ¬
              const transcript = finalTranscriptRef.current || interimTranscriptRef.current
              if (transcript.trim()) {
                hasRecognitionResultRef.current = true
                handleVoiceInput(transcript.trim())
              }
              recognition.stop()
            } catch (err) {
              console.error('è¶…æ—¶åœæ­¢å½•éŸ³å¤±è´¥:', err)
            }
          }
        }, 30000)
      } catch (err) {
        console.error('å¯åŠ¨å½•éŸ³å¤±è´¥:', err)
        let errorMessage = 'æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£æƒé™å’Œè®¾å¤‡'
        
        // å¤„ç†ç‰¹å®šé”™è¯¯ç±»å‹
        if (err instanceof Error) {
          if (err.name === 'NotReadableError') {
            errorMessage = 'éº¦å…‹é£è®¾å¤‡è¢«å ç”¨æˆ–æ•…éšœï¼Œè¯·æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–åº”ç”¨æ­£åœ¨ä½¿ç”¨éº¦å…‹é£'
          } else if (err.name === 'NotAllowedError') {
            errorMessage = 'éº¦å…‹é£æƒé™è¢«æ‹’ç»ï¼Œè¯·åœ¨æµè§ˆå™¨è®¾ç½®ä¸­å…è®¸è®¿é—®éº¦å…‹é£'
          } else if (err.name === 'NotFoundError') {
            errorMessage = 'æœªæ‰¾åˆ°éº¦å…‹é£è®¾å¤‡ï¼Œè¯·è¿æ¥éº¦å…‹é£åé‡è¯•'
          }
        }
        
        setError(errorMessage)
        setIsRecording(false)
      }
    }
  }, [recognition, isRecording, loading])

  // åœæ­¢å½•éŸ³
  const stopRecording = useCallback(() => {
    if (recognition) {
      try {
        // æ¸…é™¤æ‰€æœ‰å®šæ—¶å™¨
        if (recordingTimeoutRef.current) {
          clearTimeout(recordingTimeoutRef.current)
          recordingTimeoutRef.current = null
        }
        if (silenceTimeoutRef.current) {
          clearTimeout(silenceTimeoutRef.current)
          silenceTimeoutRef.current = null
        }
        
        // å¦‚æœæœ‰è¯†åˆ«ç»“æœï¼Œä½¿ç”¨å½“å‰çš„è½¬å½•æ–‡æœ¬
        const transcript = finalTranscriptRef.current || interimTranscriptRef.current
        if (transcript.trim()) {
          hasRecognitionResultRef.current = true
          handleVoiceInput(transcript.trim())
        }
        
        // åœæ­¢è¯­éŸ³è¯†åˆ«
        recognition.stop()
        
        // æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œè®¾ç½® isRecording = false
        // è®© onend å›è°ƒæ¥å¤„ç†ï¼Œä»¥ä¾¿æ£€æŸ¥æ˜¯å¦æœ‰è¯†åˆ«ç»“æœ
      } catch (err) {
        console.error('åœæ­¢å½•éŸ³å¤±è´¥:', err)
        // å¦‚æœåœæ­¢å¤±è´¥ï¼Œå¼ºåˆ¶é‡ç½®çŠ¶æ€
        hasRecognitionResultRef.current = false
        finalTranscriptRef.current = ''
        interimTranscriptRef.current = ''
        setIsRecording(false)
      }
    }
  }, [recognition])

  // æ’­æ”¾è¯­éŸ³
  const playAudio = (audioUrl: string, messageIndex: number) => {
    if (playingMessageIndex === messageIndex && audioElement) {
      audioElement.pause()
      setPlayingMessageIndex(null)
      setAudioElement(null)
      return
    }

    if (audioElement) {
      audioElement.pause()
    }

    const audio = new Audio(audioUrl)
    setAudioElement(audio)
    setPlayingMessageIndex(messageIndex)

    audio.play().catch(err => {
      console.error('éŸ³é¢‘æ’­æ”¾å¤±è´¥:', err)
      setPlayingMessageIndex(null)
      setAudioElement(null)
      // ä¸è®¾ç½®é”™è¯¯çŠ¶æ€ï¼Œå› ä¸ºéŸ³é¢‘æ’­æ”¾å¤±è´¥ä¸åº”è¯¥å½±å“å¯¹è¯æµç¨‹
    })

    audio.onended = () => {
      setPlayingMessageIndex(null)
      setAudioElement(null)
    }

    audio.onerror = () => {
      setPlayingMessageIndex(null)
      setAudioElement(null)
      console.error('éŸ³é¢‘åŠ è½½å¤±è´¥')
      // ä¸è®¾ç½®é”™è¯¯çŠ¶æ€ï¼Œå› ä¸ºéŸ³é¢‘åŠ è½½å¤±è´¥ä¸åº”è¯¥å½±å“å¯¹è¯æµç¨‹
    }
  }

  // ç»“æŸæµ‹è¯•
  const endTest = async () => {
    setStatus('analyzing')
    setLoading(true)

    try {
      const currentMessages = messagesRef.current
      const currentRoundValue = currentRoundRef.current

      console.log('å¼€å§‹åˆ†ææµ‹è¯•ç»“æœ...')
      console.log('å¯¹è¯å†å²:', currentMessages)

      const analysisResponse = await fetch('/api/open-test/conversation-analysis', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          conversation: currentMessages,
          rounds: currentRoundValue
        })
      })

      if (!analysisResponse.ok) {
        const errorData = await analysisResponse.json()
        console.error('æµ‹è¯•åˆ†æ API è°ƒç”¨å¤±è´¥:', errorData)
        throw new Error(errorData.error || 'æµ‹è¯•åˆ†æ API è°ƒç”¨å¤±è´¥')
      }

      const analysisData = await analysisResponse.json()
      console.log('æµ‹è¯•åˆ†æç»“æœ:', analysisData)

      setStatus('completed')
      setShowAnalysis(true)
    } catch (err) {
      console.error('ç»“æŸæµ‹è¯•å¤±è´¥:', err)
      // å³ä½¿åˆ†æå¤±è´¥ä¹Ÿæ ‡è®°ä¸ºå®Œæˆ
      setStatus('completed')
      setShowAnalysis(true)
    } finally {
      setLoading(false)
    }
  }

  // æ¸²æŸ“ä¸åŒçŠ¶æ€çš„ç•Œé¢
  const renderContent = () => {
    switch (status) {
      case 'idle':
        const hasSpeechSupport = 'webkitSpeechRecognition' in window
        return (
          <div className="text-center py-12 px-6">
            <div className="w-16 h-16 mx-auto mb-6 rounded-full bg-gradient-to-r from-[#EC4899] to-[#F472B6] flex items-center justify-center shadow-lg">
              <i className="fas fa-comments text-white text-2xl"></i>
            </div>
            <h3 className="text-lg font-semibold text-[#1F2937] mb-4">å¼€æ”¾é¢˜æµ‹è¯•</h3>
            <div className="bg-white rounded-2xl shadow-sm border border-gray-100 p-4 mb-6">
              <p className="text-sm font-medium text-[#1F2937] mb-2">{testContent.topic}</p>
              <p className="text-sm text-[#6B7280]">{testContent.description}</p>
              <p className="text-sm text-[#6B7280] mt-2">{testContent.scenario_context}</p>
            </div>
            <p className="text-sm text-[#6B7280] mb-8">
              æ‚¨å°†ä¸AIè¿›è¡Œå¤šè½®å¯¹è¯ï¼Œé€šè¿‡è¯­éŸ³å›ç­”é—®é¢˜
            </p>
            {!hasSpeechSupport && (
              <div className="bg-[#FFF8EE] border border-[#F59E0B]/20 rounded-2xl p-4 mb-8">
                <p className="text-sm text-[#92400E]">
                  <i className="fas fa-exclamation-triangle mr-2"></i>
                  æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½ï¼Œè¯·ä½¿ç”¨æ”¯æŒçš„æµè§ˆå™¨ï¼ˆå¦‚Chromeï¼‰
                </p>
              </div>
            )}
            <button
              className={`w-full py-4 rounded-2xl font-semibold text-sm transition-all ${hasSpeechSupport
                ? 'bg-gradient-to-r from-[#4F7CF0] to-[#7B5FE8] text-white shadow-md hover:shadow-lg'
                : 'bg-gray-200 text-gray-400 cursor-not-allowed'}`}
              onClick={buildQuestionAnalysisFromContent}
              disabled={!hasSpeechSupport}
            >
              å¼€å§‹æµ‹è¯•
            </button>
          </div>
        )

      case 'analyzing':
        return (
          <AnalyzingView
            message="æ­£åœ¨åˆ†æé¢˜ç›®..."
            subMessage="AI æ­£åœ¨ç†è§£åœºæ™¯ã€è§’è‰²å’Œå¯¹è¯ç›®æ ‡"
          />
        )

      case 'role-selection':
        return (
          <RoleSelectionView
            questionAnalysis={questionAnalysis}
            selectedRole={selectedRole}
            difficultyLevel={difficultyLevel}
            voiceEnabled={voiceEnabled}
            error={error}
            sceneName={testContent.topic}
            onSelectRole={setSelectedRole}
            onSelectDifficulty={setDifficultyLevel}
            onToggleVoice={() => setVoiceEnabled(!voiceEnabled)}
            onConfirm={confirmRoleAndDifficulty}
            onClearError={() => setError('')}
          />
        )

      case 'initializing':
        return (
          <InitializingView
            message="æ­£åœ¨åˆå§‹åŒ–å¯¹è¯..."
            subMessage="AI æ­£åœ¨å‡†å¤‡è§’è‰²å’Œåœºæ™¯è®¾ç½®"
          />
        )

      case 'active':
        return (
          <ActiveChatView
            messages={messages}
            currentRound={currentRound}
            maxRounds={maxRounds}
            isRecording={isRecording}
            isGeneratingResponse={isGeneratingResponse}
            playingMessageIndex={playingMessageIndex}
            error={error}
            isRoundLimitReached={isRoundLimitReached}
            onStartRecording={startRecording}
            onStopRecording={stopRecording}
            onPlayAudio={playAudio}
            onSendText={handleVoiceInput}
            onSubmitEvaluation={endTest}
            messagesEndRef={messagesEndRef}
            onBack={() => {
              // å›é€€åˆ°è§’è‰²é€‰æ‹©é¡µé¢ï¼Œæ¸…ç©ºå¯¹è¯çŠ¶æ€
              setMessages([])
              setCurrentRound(0)
              setIsRoundLimitReached(false)
              setStatus('role-selection')
            }}
          />
        )

      case 'completed':
        return (
          <AnimatePresence mode="wait">
            {showAnalysis ? (
              <TestAnalysis
                sceneId={sceneId}
                testId={testId}
                conversation={messages}
                rounds={currentRound}
                onComplete={onComplete}
              />
            ) : (
              <CompletedView
                currentRound={currentRound}
                sceneId={sceneId}
                testId={testId}
                messages={messages}
                onViewAnalysis={() => setShowAnalysis(true)}
                onComplete={onComplete}
              />
            )}
          </AnimatePresence>
        )

      default:
        return null
    }
  }

  return (
    <div className="w-full h-full flex flex-col">
      {renderContent()}
    </div>
  )
}
